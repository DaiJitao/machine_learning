import jieba_fast as jieba
import re
import json

def clean_text(text):
    text = str(text)
    # åŽ»æŽ‰urlåœ°å€
    text = re.sub('''http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+''', '', text)
    # åŽ»æŽ‰@æŸæŸæŸ  text = re.sub(r"@.{1,12}\s", "ï¼Œ", text)
    text = re.sub(r"\[.{0,5}\]|//|@.{1,15}[:\s]{1,2}", "ï¼Œ", text)
    text = re.sub("[(ã€‚ï¼Œ)(ï¼ï¼Œ)(ï¼Ÿï¼Œ)(ã€ï¼Œ)@(//@)ï½ž>\(\)]{1,}", "ï¼Œ", text)
    # å¤šä¸ªé—®å·å˜ä¸ºä¸€ä¸ªé—®å·
    text = re.sub(r"[ã€‚ï¼Ÿ\?]{2,}", "ï¼Ÿ", text)
    text = re.sub(r"[ï¼!]{2,}", "ï¼", text)
    # åŽ»æŽ‰#æŸæŸæŸ# å’Œå•ä¸ªå­—æ¯å•ä¸ªæ•°å­—
    text = re.sub(r"åŠ¨æ€|è½¬å‘|å¾®åš|å¾®åšè½¬|è½¬å‘å¾®åš|#[^#]{1,10}#|å±•å¼€å…¨æ–‡|([a-zA-Z]|[0-9])", "ï¼Œ", text)
    # ï¼Ÿ? -> ?
    text = re.sub(r"(\?ï¼Ÿ)|(ï¼Ÿ\?)", "ï¼Ÿ", text)
    text = re.sub(r"â€”{2,}|-{2,}", "â€”", text)
    text = re.sub(r"â€¦{1,}|\.{4,}", "â€¦", text)
    r = u'[/ã€ã€‘â—â– ï¿½â†’ï¼Žãƒ»ðŸ‡¨ðŸ‡³ï¼™ï¼˜ï¼—ï¼–ï¼•ï¼”ï¼“ï¼’ï¼‘ï¼ï¼ï¼…ï¼»ï¼½Ã—\[\]^_`{|}~(:Ð·ã€âˆ )]'
    text = re.sub(r, "ï¼Œ", text)
    # åŽ»æŽ‰éžä¸­æ–‡
    not_chinese = r'[^\u4e00-\u9fa5ï¼Œ{}ã€ã€‘ï¼ˆï¼‰ã€‚ï¼šï¼›â€œâ€˜â€â€¦ã€Šã€‹ã€ï¼ï¼Ÿâ€”â€”]'
    text = re.sub(not_chinese, " ", text)
    text = re.sub(u".[å²ä¸ªå¹´åª]çš„", "ã€‚", text)
    # å¤šä¸ªç©ºæ ¼å˜ä¸º1ä¸ª
    text = re.sub(r"[ï¼Œï¼ã€‚ã€â€˜â€™ï¼›ï¼š\s]{1,}", " ", text)
    text = re.sub(r"[(\sï¼Œ)(\s,),ï¼Œ(ï¼ˆï¼‰)(ï¼šï¼Œ)(ï¼š ï¼š)(â€”ï¼Œ)]{1,}", "ï¼Œ", text).strip()
    text = re.sub(u"[å¨å“ˆå‘µå•Šæ“è‰¹]{1,8}[ï¼Œã€‚ï¼Ÿã€ï¼â€˜â€™ï¼›ï¼šÂ·]{0,2}", "ã€‚", text)
    text = re.sub(r"^[ï¼Œï¼ã€‚â€˜â€™ï¼Ÿã€]{1}\b", "", text)
    return text


def clean_cutwords(word):
    if len(word.strip()) == 2:
        word = re.sub(r"[å‰ä¸Šä¸‹åˆä¸­ä»Šæ˜ŽåŽ][åŽå¹´æ—¥æœˆå¤©åˆ]|ä½äºŽ|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ä¸Šä¸‹æ¬¡æ¡å¼ ä¹‹åªæžä»¶ä¸‡åƒä¸ªç‰‡é¦–ç§å—å¥—åŒç»„æ®µåº§ç¥¨æ ¹å£å¼¯æ¹¾å¤´å¯¹ç±³ä½ç¯‡å¶æ—¥æœˆå…‹é¡¿å¨æŽ’å±‚é¢åŒ…åœˆå¤©]", "", word)
    if len(word.strip()) == 3:
        word = re.sub(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][æ¬¡æ¡å¼ ä¹‹åªæžä»¶ä¸‡åƒä¸ªç‰‡é¦–ç§å—å¥—åŒç»„æ®µåº§ç¥¨æ ¹å£å¼¯æ¹¾å¤´å¯¹ç±³ä½ç¯‡å¶æ—¥æœˆå…‹é¡¿å¨æŽ’å±‚é¢åŒ…åœˆå¤©]", "", word)

    return word


if __name__ == '__main__':

    userwords = [line.rstrip() for line in open('./data/userdict.txt', encoding='utf-8')]
    for word in userwords:
        jieba.add_word(word.strip())
    stopwords = [line.rstrip() for line in open('./data/stopwords.txt', encoding='utf-8')]
    # with open("./data/stopwords1.txt", mode="w+", encoding="utf-8") as fp:
    #     s = set(stopwords)
    #     fp.write("\n".join(s))

    file = "./data/hotword.txt"
    with open(file, mode="r", encoding="utf-8") as fp:
        i = 0
        for line in fp.readlines()[0:200]:
            # if i == 8: break
            d = json.loads(line)['content']
            print(i )
            print(d)
            cleanText = clean_text(d)
            print("-"*60 + "æ¸…æ´—ä¹‹åŽï¼š")
            print(cleanText)
            c = [word for word in jieba.cut(cleanText) if len(word.strip()) > 1]
            print()
            print(" ".join(c))
            # print("removed Stopwprds:")
            lst = [word for word in c if word not in stopwords]
            print(" ".join(lst))
            print("===============================\n\n")
            i += 1
            break

    text = "ä»Šå¤©"
    print(text)
    text = clean_cutwords(text)
    print(text)
