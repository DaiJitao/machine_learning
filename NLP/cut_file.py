import jieba_fast as jieba
from pyhanlp import HanLP
import re
import json
from typing import List


def clean_text(text):
    text = str(text)
    # å»æ‰urlåœ°å€
    text = re.sub('''http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+''', '', text)
    # å»æ‰@æŸæŸæŸ  text = re.sub(r"@.{1,12}\s", "ï¼Œ", text)
    text = re.sub(r"//@[^@]{1,20}:\s{0,1}|\[.{1,5}\]|@[^@]{1,}:\s{0,1}|@.{1,10}\s", "ï¼Œ", text)
    text = re.sub(r"@[^a].{1,10}", "ï¼Œ", text)
    # text = re.sub(r"\[.{0,5}\]|//@.{1,15}[:\s]{1,2}|@.{1,15}[:\s]{1,2}", "ï¼Œ", text)
    text = re.sub(r"åˆ†äº«æ¥è‡ªï¼š.{1,6}\s", "", text)
    text = re.sub(r"(ï¼Ÿï¼Œ){1,3}|(ã€‚ï¼Œ){1,3}|(ï¼ï¼Œ){1,6}", "ã€‚", text)
    # å¤šä¸ªé—®å·å˜ä¸ºä¸€ä¸ªé—®å·
    text = re.sub(r"[ï¼!ã€‚ï¼Ÿ\?]{2,}", "ï¼", text)
    # å»æ‰#æŸæŸæŸ# å’Œå•ä¸ªå­—æ¯å•ä¸ªæ•°å­—
    text = re.sub(r"åŠ¨æ€|è½¬å‘|å¾®åš|å¾®åšè½¬|è½¬å‘å¾®åš|#[^#]{1,10}#|å±•å¼€å…¨æ–‡|([a-zA-Z]|[0-9])", "ï¼Œ", text)
    # ï¼Ÿ? -> ?
    text = re.sub(r"(\?ï¼Ÿ)|(ï¼Ÿ\?)", "ï¼Ÿ", text)
    text = re.sub(r"â€”{2,}|-{2,}", "â€”", text)
    text = re.sub(r"â€¦{1,}|\.{4,}", "â€¦", text)
    r = u'[/ã€ã€‘â—â– ï¿½â†’ï¼ãƒ»ğŸ‡¨ğŸ‡³ï¼™ï¼˜ï¼—ï¼–ï¼•ï¼”ï¼“ï¼’ï¼‘ï¼ï¼ï¼…ï¼»ï¼½Ã—\[\]^_`{|}~(:Ğ·ã€âˆ )]'
    text = re.sub(r, "ï¼Œ", text)
    # å»æ‰éä¸­æ–‡
    not_chinese = r'[^\u4e00-\u9fa5ï¼Œ{}ã€ã€‘ï¼ˆï¼‰ã€‚ï¼šï¼›â€œâ€˜â€â€¦ã€Šã€‹ã€ï¼ï¼Ÿâ€”â€”]'
    text = re.sub(not_chinese, " ", text)
    text = re.sub(u".[å²ä¸ªå¹´åª]çš„", "ã€‚", text)
    # å¤šä¸ªç©ºæ ¼å˜ä¸º1ä¸ª
    text = re.sub(r"[ï¼Œï¼ã€‚ã€â€˜â€™ï¼›ï¼š\s]{2,6}", "ã€‚", text)
    # text = re.sub(r"[(\sï¼Œ),ï¼ˆï¼‰ï¼š â€”]{1,}", "ã€‚", text).strip()
    text = re.sub(u"[å¸…å¨å“ˆå‘µå‘€å“å˜»å•Šæ“è‰¹]{1,8}[ï¼Œã€‚ï¼Ÿã€ï¼â€˜â€™ï¼›ï¼šÂ·]{0,2}", "ã€‚", text)
    text = re.sub(r"^[ï¼Œï¼ã€‚â€˜â€™ï¼Ÿã€]{1}\b", "", text)
    return text


def clean_cutwords(word):
    if word:
        word = str(word)
        if len(word.strip()) == 2:
            word = re.sub(
                r"[å‰ä¸Šä¸‹åˆä¸­ä»Šæ˜å][åå¹´æ—¥æœˆå¤©åˆ]|ä½äº|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]|[æœŸè¿™é‚£ä¸Šä¸‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åå¤š][æœŸç§’ä¸Šä¸‹æ¬¡æ¡å¼ ä¹‹åªæœµæä»¶ä¸‡åƒä¸ªç‰‡é¦–ç§å—å¥—åŒç»„æ®µåº§ç¥¨æ ¹å£å¼¯æ¹¾å¤´å¯¹ç±³ä½ç¯‡å¶æ—¥æœˆå…‹é¡¿å¨æ’å±‚é¢åŒ…åœˆå¤©å¹´]",
                "", word)
            return word
        elif len(word.strip()) == 3:
            word = re.sub(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][æœŸç§’æ¬¡æ¡å¼ ä¹‹åªæœµæä»¶ä¸‡åƒä¸ªç‰‡é¦–ç§å—å¥—åŒç»„æ®µåº§ç¥¨æ ¹å£å¼¯æ¹¾å¤´å¯¹ç±³ä½ç¯‡å¶æ—¥æœˆå…‹é¡¿å¨æ’å±‚é¢åŒ…åœˆå¤©å¹´]", "", word)
            return word
        else:
            return word

    return ''


def user_dict(outfile):
    s = set([line.strip() for line in open('./data/userdict.txt', encoding='utf-8')])
    s = [word + " 100 n" for word in s]
    with open(outfile, mode='w', encoding='utf-8') as fp:
        fp.write("\n".join(s))


def load_hanlp_nature(file: str = "./data/hanlp/hanlp_word_nature.txt") -> dict:
    """
    è·å–hanlpè¯æ€§
    :param file:
    :return: dict()
    """
    with open(file, mode="r", encoding="utf-8") as fp:
        result = {}
        lines = fp.readlines()
        for line in lines:
            wn = line.strip().split(" ")
            result[wn[0]] = wn[-1]
    return result


def hanLp_cut(text, isNature=True):
    natures = load_hanlp_nature()
    r = []
    words = HanLP.segment(text)
    for w in words:
        word = w.word
        if isNature:
            nture = w.nature
            chN = natures[nture.toString()]
            r.append(word + ":( "+nture.toString() + chN + ")")
        else:
            r.append(word.strip())
    return r


if __name__ == '__main__1':
    out_file = "./data/userdict1.txt"
    user_dict(out_file)

if __name__ == '__main__':

    jieba.load_userdict('./data/userdict.txt')
    stopwords = [line.rstrip() for line in open('./data/stopwords.txt', encoding='utf-8')]

    file = "./data/hotword.txt"
    with open(file, mode="r", encoding="utf-8") as fp:
        i = 0
        for line in fp.readlines()[:]:
            text = json.loads(line)['content']
            print(i)
            cleanedText = clean_text(text)
            print("-" * 60 + "æ¸…æ´—ä¹‹åï¼š")
            print(cleanedText)
            c = [word for word in jieba.cut(cleanedText)]
            print("jiebaåˆå§‹åˆ†è¯ï¼š", " ".join(c))
            print("removed Stopwprds:")
            lst1 = [word.strip() for word in c if word.strip() not in stopwords]
            print("jiebaæœ€ç»ˆåˆ†è¯ï¼š", " ".join(lst1))
            hanlpWords = hanLp_cut(cleanedText,False)
            lst1 = [word for word in hanlpWords if word not in stopwords]
            print("hanlpæœ€ç»ˆåˆ†è¯ï¼š", " ".join(lst1))
            print("===============================\n\n")
            i += 1
            if i == 1: break

    text = "æ­£æœˆåˆå››åˆäº”"
    print(text)

    text = [w for w in jieba.cut(text)]
    print(text)
