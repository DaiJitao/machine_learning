import jieba_fast as jieba
import re
import json


def clean_text(text):
    text = str(text)
    # å»æ‰urlåœ°å€
    text = re.sub('''http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+''', '', text)
    # å»æ‰@æŸæŸæŸ  text = re.sub(r"@.{1,12}\s", "ï¼Œ", text)
    text = re.sub(r"//@[^@]{1,20}:\s{0,1}|\[.{1,5}\]|@[^@]{1,}:\s{0,1}|@.{1,10}\s", "ï¼Œ", text)
    text = re.sub(r"@[^a].{1,10}","ï¼Œ", text)
    # text = re.sub(r"\[.{0,5}\]|//@.{1,15}[:\s]{1,2}|@.{1,15}[:\s]{1,2}", "ï¼Œ", text)
    text = re.sub(r"åˆ†äº«æ¥è‡ªï¼š.{1,6}\s", "", text)
    text = re.sub(r"(ï¼Ÿï¼Œ){1,3}|(ã€‚ï¼Œ){1,3}|(ï¼ï¼Œ){1,6}","ã€‚", text)
    # å¤šä¸ªé—®å·å˜ä¸ºä¸€ä¸ªé—®å·
    text = re.sub(r"[ï¼!ã€‚ï¼Ÿ\?]{2,}", "ï¼", text)
    # å»æ‰#æŸæŸæŸ# å’Œå•ä¸ªå­—æ¯å•ä¸ªæ•°å­—
    text = re.sub(r"åŠ¨æ€|è½¬å‘|å¾®åš|å¾®åšè½¬|è½¬å‘å¾®åš|#[^#]{1,10}#|å±•å¼€å…¨æ–‡|([a-zA-Z]|[0-9])", "ï¼Œ", text)
    # ï¼Ÿ? -> ?
    text = re.sub(r"(\?ï¼Ÿ)|(ï¼Ÿ\?)", "ï¼Ÿ", text)
    text = re.sub(r"â€”{2,}|-{2,}", "â€”", text)
    text = re.sub(r"â€¦{1,}|\.{4,}", "â€¦", text)
    r = u'[/ã€ã€‘â—â– ï¿½â†’ï¼ãƒ»ğŸ‡¨ğŸ‡³ï¼™ï¼˜ï¼—ï¼–ï¼•ï¼”ï¼“ï¼’ï¼‘ï¼ï¼ï¼…ï¼»ï¼½Ã—\[\]^_`{|}~(:Ğ·ã€âˆ )]'
    text = re.sub(r, "ï¼Œ", text)
    # å»æ‰éä¸­æ–‡
    not_chinese = r'[^\u4e00-\u9fa5ï¼Œ{}ã€ã€‘ï¼ˆï¼‰ã€‚ï¼šï¼›â€œâ€˜â€â€¦ã€Šã€‹ã€ï¼ï¼Ÿâ€”â€”]'
    text = re.sub(not_chinese, " ", text)
    text = re.sub(u".[å²ä¸ªå¹´åª]çš„", "ã€‚", text)
    # å¤šä¸ªç©ºæ ¼å˜ä¸º1ä¸ª
    text = re.sub(r"[ï¼Œï¼ã€‚ã€â€˜â€™ï¼›ï¼š\s]{2,6}", "ã€‚", text)
    # text = re.sub(r"[(\sï¼Œ),ï¼ˆï¼‰ï¼š â€”]{1,}", "ã€‚", text).strip()
    text = re.sub(u"[å¸…å¨å“ˆå‘µå‘€å“å˜»å•Šæ“è‰¹]{1,8}[ï¼Œã€‚ï¼Ÿã€ï¼â€˜â€™ï¼›ï¼šÂ·]{0,2}", "ã€‚", text)
    text = re.sub(r"^[ï¼Œï¼ã€‚â€˜â€™ï¼Ÿã€]{1}\b", "", text)
    return text


def clean_cutwords(word):
    if word:
        word = str(word)
        if len(word.strip()) == 2:
            word = re.sub(
                r"[å‰ä¸Šä¸‹åˆä¸­ä»Šæ˜å][åå¹´æ—¥æœˆå¤©åˆ]|ä½äº|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]|[æœŸè¿™é‚£ä¸Šä¸‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åå¤š][æœŸç§’ä¸Šä¸‹æ¬¡æ¡å¼ ä¹‹åªæœµæä»¶ä¸‡åƒä¸ªç‰‡é¦–ç§å—å¥—åŒç»„æ®µåº§ç¥¨æ ¹å£å¼¯æ¹¾å¤´å¯¹ç±³ä½ç¯‡å¶æ—¥æœˆå…‹é¡¿å¨æ’å±‚é¢åŒ…åœˆå¤©å¹´]",
                "", word)
            return word
        elif len(word.strip()) == 3:
            word = re.sub(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][æœŸç§’æ¬¡æ¡å¼ ä¹‹åªæœµæä»¶ä¸‡åƒä¸ªç‰‡é¦–ç§å—å¥—åŒç»„æ®µåº§ç¥¨æ ¹å£å¼¯æ¹¾å¤´å¯¹ç±³ä½ç¯‡å¶æ—¥æœˆå…‹é¡¿å¨æ’å±‚é¢åŒ…åœˆå¤©å¹´]", "", word)
            return word
        else:
            return word

    return ''


def user_dict(outfile):
    s = set([line.strip() for line in open('./data/userdict.txt', encoding='utf-8')])
    s = [word + " 100 n" for word in s]
    with open(outfile, mode='w', encoding='utf-8') as fp:
        fp.write("\n".join(s))


if __name__ == '__main__1':
    out_file = "./data/userdict1.txt"
    user_dict(out_file)

if __name__ == '__main__':

    jieba.load_userdict('./data/userdict.txt')
    stopwords = [line.rstrip() for line in open('./data/stopwords.txt', encoding='utf-8')]

    file = "./data/hotword.txt"
    with open(file, mode="r", encoding="utf-8") as fp:
        i = 0
        for line in fp.readlines()[:]:
            d = json.loads(line)['content']
            print(i)
            print(d)
            cleanText = clean_text(d)
            print("-" * 60 + "æ¸…æ´—ä¹‹åï¼š")
            print(cleanText)
            c = [word for word in jieba.cut(cleanText) if len(word.strip()) > 1]
            print()
            print("åˆå§‹åˆ†è¯ï¼š", " ".join(c))
            print("removed Stopwprds:")
            lst1 = [word for word in c if word not in stopwords]
            print("æœ€ç»ˆåˆ†è¯ï¼š", " ".join(lst1))
            print("===============================\n\n")
            i += 1

    text = "æ­£æœˆåˆå››åˆäº”"
    print(text)

    # jieba.load_userdict('./data/userdict.txt')
    jieba.load_userdict('./data/supplement.txt')

    text = [w for w in jieba.cut(text)]
    print(text)
