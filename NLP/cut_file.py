import jieba_fast as jieba
import re
import json

def clean_text(text):
    text = str(text)
    # å»æ‰urlåœ°å€
    text = re.sub('''http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+''', '', text)
    # å»æ‰@æŸæŸæŸ
    text = re.sub(r"\[.{0,5}\]|//|@.{1,8}:{0,1}", "ï¼Œ", text)
    # text = re.sub('''@.+@.+([\n]|[\t])''', '', text)
    # text = re.sub("(ã€ï¼Œ)|@|//@|ï½|>{1,}|\(\)", "ï¼Œ", text)
    text = re.sub("[(ã€‚ï¼Œ)(ï¼ï¼Œ)(ï¼Ÿï¼Œ)(ã€ï¼Œ)@(//@)ï½>\(\)]{1,}", "ï¼Œ", text)
    # å¤šä¸ªé—®å·å˜ä¸ºä¸€ä¸ªé—®å·
    text = re.sub(r"[ã€‚ï¼Ÿ\?]{2,}", "ï¼Ÿ", text)
    text = re.sub(r"[ï¼!]{2,}", "ï¼", text)
    # å»æ‰#æŸæŸæŸ# å’Œå•ä¸ªå­—æ¯å•ä¸ªæ•°å­—
    text = re.sub("è½¬å‘å¾®åš|#.{1,20}#|å±•å¼€å…¨æ–‡|([a-zA-Z]|[0-9])", "ï¼Œ", text)
    # ï¼Ÿ? -> ?
    text = re.sub(r"(\?ï¼Ÿ)|(ï¼Ÿ\?)", "ï¼Ÿ", text)
    text = re.sub(r"â€”{2,}|-{2,}", "â€”", text)
    text = re.sub(r"â€¦{1,}|\.{4,}", "â€¦", text)
    r = u'[/ã€ã€‘â—â– ï¿½â†’ï¼ãƒ»ğŸ‡¨ğŸ‡³ï¼™ï¼˜ï¼—ï¼–ï¼•ï¼”ï¼“ï¼’ï¼‘ï¼ï¼ï¼…ï¼»ï¼½Ã—\[\]^_`{|}~(:Ğ·ã€âˆ )]'
    text = re.sub(r, "ï¼Œ", text)
    # å»æ‰éä¸­æ–‡
    not_chinese = r'[^\u4e00-\u9fa5ï¼Œ{}ã€ã€‘ï¼ˆï¼‰ã€‚ï¼šï¼›â€œâ€˜â€â€¦ã€Šã€‹ã€ï¼ï¼Ÿâ€”â€”]'
    text = re.sub(not_chinese, " ", text)
    # å¤šä¸ªç©ºæ ¼å˜ä¸º1ä¸ª
    text = re.sub(r"\s{1,}", " ", text)
    text = re.sub(r"[(\sï¼Œ)(\s,),ï¼Œ(ï¼ˆï¼‰)(ï¼šï¼Œ)(ï¼š ï¼š)(â€”ï¼Œ)]{1,}", "ï¼Œ", text).strip()
    return text

if __name__ == '__main__':

    userwords = [line.rstrip() for line in open('./data/userdict.txt', encoding='utf-8')]
    for word in userwords:
        jieba.add_word(word.strip())
    stopwords = [line.rstrip() for line in open('./data/stopwords.txt', encoding='utf-8')]
    # with open("./data/stopwords1.txt", mode="w+", encoding="utf-8") as fp:
    #     s = set(stopwords)
    #     fp.write("\n".join(s))

    file = "./data/hotdata.txt"
    with open(file, mode="r", encoding="utf-8") as fp:
        i = 0
        for line in fp.readlines()[43:]:
            # if i == 8: break

            d = json.loads(line)['content']
            print(i )
            print(d)
            cleanText = clean_text(d)
            print("-"*60 + "æ¸…æ´—ä¹‹åï¼š")
            print(cleanText)
            c = [word for word in jieba.cut(cleanText)]
            print()
            print(" ".join(c))
            # print("removed Stopwprds:")
            lst = [word for word in c if word not in stopwords]
            print(" ".join(lst))
            print("===============================\n\n")
            i += 1

    text = "ğŸ“è‹å·æ‹™æ”¿å›­  è‹å·å›­æ—çš„ä»£è¡¨ä½œã€‚å…æ¦­ç²¾ç¾ï¼Œå±±æ°´è¦ç»•ï¼Œå…·æœ‰æµ“éƒçš„æ±Ÿå—æ°´ä¹¡ç‰¹è‰²ã€‚  #é‡è§ç¾å¥½##å¸¦ç€å¾®åšå»æ—…æ¸¸##è¡Œæ‘„æ±Ÿè‹##å¯»æ‰¾æ±Ÿè‹æœ€ç¾é£æ™¯# "
    # text = re.sub(r"(#.{1,20}#)", " ", text)
    # print(text)
    text = re.sub("è½¬å‘å¾®åš|#.{1,20}#|å±•å¼€å…¨æ–‡|([a-zA-Z]|[0-9])", "", text)

    print(text)
