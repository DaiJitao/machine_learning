"""
ç–«æƒ…æœŸé—´ç½‘æ°‘æƒ…ç»ªè¯†åˆ«
"""

import numpy as np
import jieba
import pandas as pd
import re

jieba.load_userdict("./data/user_dict.txt")


def clean_data(file, out_csv):
    data_df = pd.read_csv(file, encoding='utf-8')
    content = data_df['å¾®åšä¸­æ–‡å†…å®¹']
    label = data_df['æƒ…æ„Ÿå€¾å‘']
    content_cleaned = content.apply(clean_text)
    new_df = pd.DataFrame({"å¾®åšä¸­æ–‡å†…å®¹": content, "æ¸…æ´—åå†…å®¹": content_cleaned, "æƒ…æ„Ÿå€¾å‘": label})
    new_df.to_csv(out_csv, encoding='utf-8', index=False)


def clean_text(text):
    text = str(text)
    # å»æ‰urlåœ°å€
    text = re.sub('''http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+''', '', text)
    # å»æ‰@æŸæŸæŸ
    text = re.sub('''@.+@.+([\n]|[\t])''', '', text)
    text = re.sub("//@|ï½|>{1,}|\(\)|[@Â·ã€ã€]", " ", text)
    # å¤šä¸ªé—®å·å˜ä¸ºä¸€ä¸ªé—®å·
    text = re.sub(r"[ã€‚ï¼Ÿ\?]{2,}", "ï¼Ÿ", text)
    text = re.sub(r"ï¼{2,}", "ï¼", text)
    # å»æ‰#æŸæŸæŸ# å’Œå•ä¸ªå­—æ¯å•ä¸ªæ•°å­—
    text = re.sub("#.{1,6}#|å±•å¼€å…¨æ–‡|([a-zA-Z]|[0-9])", "", text)
    # ï¼Ÿ? -> ?
    text = re.sub(r"(\?ï¼Ÿ)|(ï¼Ÿ\?)", "ï¼Ÿ", text)
    text = re.sub(r"â€”{2,}|-{2,}", "â€”", text)
    text = re.sub(r"â€¦{1,}|\.{4,}", "â€¦", text)
    r = u'[/ã€ã€‘â—â– ï¿½â†’ï¼ãƒ»ğŸ”ï¼ˆï¼™ï¼˜ï¼—ï¼–ï¼•ï¼”ï¼“ï¼’ï¼‘ï¼ï¼ï¼…ï¼»ï¼½Ã—\[\]^_`{|}~(:Ğ·ã€âˆ )ï¼‰â‘ â‘¡â‘¢â‘£]|(#|â†“|(-  -)){1,}'
    text = re.sub(r, " ", text)

    # å»æ‰ç‰¹æ®Šæ ‡ç‚¹ç¬¦å·
    # r = u'[!"#$%&\'()*+,-./:;<=>ã€Šï¿¥â˜…â–¼ã€‹ï¼Œã€‚Â·â€œâ€ï¼ˆï¼‰ã€ï¼›ï¼šï¼Ÿã€ã€‘â€”ï¼â—â– ï¿½0123456789ï¼ãƒ»ï¼™ï¼˜ï¼—ï¼–ï¼•ï¼”ï¼“ï¼’ï¼‘ï¼ï¼ï¼…ï¼»ï¼½Ã—â€¦?@[\\]^_`{|}~]'
    # text = re.sub(r, '', text)
    return text


def stop_word(file):
    with open(file, encoding="utf-8", mode="r") as f:
        d = f.read()
        stopWords = set(d.split("\n"))
    return list(stopWords)


def remove_word(src_word, stop_word):
    '''
    å»é™¤åœç”¨è¯
    :param src_word:
    :param stop_word:
    :return:
    '''
    return [word for word in src_word if word not in stop_word]


def cut_words(text):
    text = str(text).strip()
    text = re.sub("\ue627", "", text)
    text = re.sub(r"[â˜…â˜†]{1,}", "", text)
    if len(text) == 0 or len(text) == 1:
        return text
    cut_word = jieba.cut(text)
    src_word = [word for word in cut_word if len(word.strip()) > 0]
    words = remove_word(src_word, stop_word)
    text = " ".join(words)
    return text


def cut_data(file, out_csv):
    data_df = pd.read_csv(file, encoding='utf-8')
    content = data_df['æ¸…æ´—åå†…å®¹']
    label = data_df['æƒ…æ„Ÿå€¾å‘']
    cut_content = content.apply(cut_words)
    new_df = pd.DataFrame({"åˆ†è¯": cut_content, "æƒ…æ„Ÿå€¾å‘": label})
    new_df.to_csv(out_csv, encoding='utf-8', index=False)


def get_fastText_data(train_data, out_file):
    data_df = pd.read_csv(train_data, encoding='utf-8')
    cut_content = data_df["åˆ†è¯"]
    label = data_df['æƒ…æ„Ÿå€¾å‘']
    with open(out_file, mode='a', encoding='utf-8') as file:
        for words, cls in zip(cut_content, label):
            words = str(words).strip()
            cls = str(cls).strip()
            line = "__label__" + str(cls) + " " + words + "\n"
            file.write(line)
    print("æ•°æ®å¤„ç†å®Œæ¯•ï¼")

def split_data_train_test(data_file, rows, out_train_file, out_test_file, split_rate=0.8 ):
    """

    :param data_file:
    :param out_train_file:
    :param rows: æ•°æ®é›†æ€»è¡Œæ•°
    :param out_test_file:
    :param split_rate: æ•°æ®åˆ’åˆ†æ¯”ä¾‹ï¼›0.8ä»£è¡¨ç™¾åˆ†ä¹‹å…«åè®­ç»ƒé›†
    :return:
    """
    index = int(rows * split_rate)
    out_train_data = open(out_train_file, encoding='utf-8', mode='a+')
    out_test_data = open(out_test_file, mode="a+", encoding="utf-8")
    with open(data_file, mode='r', encoding="utf-8") as file:
        lines = file.readlines()
        i = 0
        for line in lines:
            if i < index:
                out_train_data.write(line + "\n")
            else:
                out_test_data.write(line + "\n")
            i += 1
    out_test_data.close()
    out_train_data.close()





if __name__ == '__main__':
    base_dir = r"F:\NLP\æŠ¥åæ¯”èµ›\ç–«æƒ…æœŸé—´ç½‘æ°‘æƒ…ç»ªè¯†åˆ«\train_ dataset"
    file_train_csv = base_dir + r"\nCoV_100k_train.labled.csv"
    file_train_cleaned_csv = base_dir + r"\cleaned_nCoV_100k_train.labled.csv"
    file_train_cut_csv = base_dir + r"\cutOK_nCoV_100k_train.labled.csv"
    # load_data(file=file_train_csv, out_csv=file_train_cleaned_csv)
    # stop_word_file = r"F:\pycharm_workspace\mygithub\machine_learning\data\StopWords.txt"
    # stop_word = stop_word(stop_word_file)
    # cut_data(file_train_cleaned_csv, file_train_cut_csv)
    train_data = file_train_cut_csv
    out_file = base_dir + r"\train_data_all.txt"
    # get_fastText_data(train_data, out_file)
    split_data_train_test(data_file=out_file, rows=100000, out_train_file=base_dir+r"\train_data.txt", out_test_file=base_dir + r"\test_data.txt")

